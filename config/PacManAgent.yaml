behaviors:
    PacManAgent:
        trainer_type: ppo

        hyperparameters:
            # Hyperparameters common to PPO and SAC
            batch_size: 256
            buffer_size: 20480
            learning_rate: 0.005
            learning_rate_schedule: linear

            # PPO-specific hyperparameters
            beta: 5.0e-3
            beta_schedule: linear
            epsilon: 0.4
            epsilon_schedule: linear
            lambd: 0.95
            num_epoch: 3
            shared_critic: true

        # Configuration of the neural network (common to PPO/SAC)
        network_settings:
            vis_encode_type: nature_cnn
            normalize: false
            hidden_units: 256
            num_layers: 3
            # memory
            memory:
                sequence_length: 64
                memory_size: 128

        # Trainer configurations common to all trainers
        max_steps: 1.0e10
        time_horizon: 64
        summary_freq: 10000
        keep_checkpoints: 320
        checkpoint_interval: 25000
        threaded: true

        reward_signals:
            # environment reward (default)
            extrinsic:
                strength: 1.0
                gamma: 0.8

            # curiosity module
            curiosity:
                strength: 0.02
                gamma: 0.8
                learning_rate: 3.0e-4

env_settings:
    env_path: build
    num_envs: 20

torch_settings:
    device: cuda

engine_settings:
    width: 350
    height: 400
    time_scale: 1
    target_frame_rate: 30
    no_graphics: false
